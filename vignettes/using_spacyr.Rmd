---
title: "A Guide to Using spacyr"
author: "Kenneth Benoit and Akitaka Matsuo"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using spacyr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##"
)
```

## A walkthrough of **spacyr**

### Starting a **spacyr** session

To allow R to access the underlying Python functionality, it must open a connection by being initialized within your R session.

We provide a function for this, `spacy_initialize()`, which attempts to make this process as painless as possible. When spaCy has been installed in a conda environment with `spacy_install()`, `spacy_initialize()` automatically detects it and initializes spaCy. If spaCy is installed in a normal environment (i.e. not in a condaenv or virtualenv), `spacy_initialize()` searches your system for Python executables, and testing which have spaCy installed.

For power users with a specialized setup of spaCy (i.e. users who have a conda environment already set up for spaCy), it is possible to specify which environment or python executable to be used through one of the following methods:

1. `condaenv` argument: supplying the name of conda environment
2. `virtualenv` argument: supplying the path to the python virtual environment
3. `python_executable` argument: supplying the path to the python

```{r}
library("spacyr")
spacy_initialize()
```

### Tokenizing and tagging texts

The `spacy_parse()` function is **spacyr**'s main workhorse.  It calls spaCy both to tokenize and tag the texts.  It provides two options for part of speech tagging, plus options to return word lemmas, entity recognition, noun phrase recognition, and dependency parsing.  It returns a `data.frame` corresponding to the emerging [*text interchange format*](https://github.com/ropensci/tif) for token data.frames.

The approach to tokenizing taken by spaCy is inclusive: it includes all tokens without restrictions, including punctuation characters and symbols.

Example:
```{r}
txt <- c(d1 = "spaCy excels at large-scale information extraction tasks.",
         d2 = "Mr. Smith goes to North Carolina.")

# process documents and obtain a data.table
parsedtxt <- spacy_parse(txt)
parsedtxt
```

Two fields are available for part-of-speech tags.  The `pos` field returned is the [Universal tagset for parts-of-speech](http://universaldependencies.org/u/pos/all.html), a general scheme that most users will find serves their needs, and also that provides equivalencies across languages.   **spacyr** also provides a more detailed tagset, defined in each spaCy language model.  For English, this is the [OntoNotes 5 version of the Penn Treebank tag set](https://spacy.io/docs/usage/pos-tagging#pos-tagging-english).
```{r}
spacy_parse(txt, tag = TRUE, entity = FALSE, lemma = FALSE)
```

For the German language model, the Universal tagset (`pos`) remains the same, but the detailed tagset (`tag`) is the [TIGER Treebank](https://spacy.io/docs/usage/pos-tagging#pos-tagging-german) scheme.


### Extracting entities

**spacyr** can extract entities, either named or ["extended"](https://spacy.io/docs/usage/entity-recognition#entity-types) from the output of `spacy_parse()`.
```{r}
parsedtxt <- spacy_parse(txt, lemma = FALSE)
entity_extract(parsedtxt)
```

```{r}
entity_extract(parsedtxt, type = "all")
```

Or, convert multi-word entities into single "tokens":
```{r}
entity_consolidate(parsedtxt)
```


### Noun phrase extraction

In a similar manner to named entity extraction, **spacyr** can extract or concatenate noun phrases (or [noun chunks](https://spacy.io/usage/linguistic-features#noun-chunks)).

```{r}
parsedtxt <- spacy_parse(txt, lemma = FALSE, nounphrase = TRUE)
nounphrase_extract(parsedtxt)
```

```{r}
nounphrase_consolidate(parsedtxt)
```



### Dependency parsing

Detailed parsing of syntactic dependencies is possible with the `dependency = TRUE` option:
```{r}
spacy_parse(txt, dependency = TRUE, lemma = FALSE, pos = FALSE)
```

### Additional token attributes

You can also extract additional [attributes of spaCy tokens](https://spacy.io/api/token#attributes) with the `additional_attributes` option:
```{r}
spacy_parse(txt, additional_attributes = c("is_title", "is_punct"))
```


### Using other language models

By default, **spacyr** loads an English language model. You also can load SpaCy's other [language models](https://spacy.io/docs/usage/models) or use one of the [language models with alpha support](https://spacy.io/docs/api/language-models#alpha-support) by specifying the `model` option when calling `spacy_initialize()`. We have successfully tested following language models with spaCy version 2.0.18.

```{r echo = FALSE}
knitr::kable(data.frame(Language = c("German", "Spanish", "Portuguese",  "French", "Italian", "Dutch"),
           ModelName = c("`de`", "`es`", "`pt`", "`fr`", "`it`", "`nl`")) )
```


This is an example of parsing German texts.
```{r}
## first finalize the spacy if it's loaded
spacy_finalize()
spacy_initialize(model = "de")

txt_german <- c(R = "R ist eine freie Programmiersprache für statistische Berechnungen und Grafiken. Sie wurde von Statistikern für Anwender mit statistischen Aufgaben entwickelt.",
               python = "Python ist eine universelle, üblicherweise interpretierte höhere Programmiersprache. Sie will einen gut lesbaren, knappen Programmierstil fördern.")
results_german <- spacy_parse(txt_german, dependency = TRUE, lemma = FALSE, tag = TRUE)
results_german
```
Note that the additional language models must first be installed in spaCy.  The German language model, for example, can be installed (`python -m spacy download de`) before you call `spacy_initialize()`.

### Additional functionalities of **spacyr**

**spacyr** package provides more functionalities that directly work with spaCy

- `spacy_tokenize()`: a function for efficient tokenization (without POS tagging, dependency parsing, lemmatization, or named entity recognition) of texts
- `spacy_extract_entity()`: a one-step function to extract named entities without `spacy_parse()`
- `spacy_extract_nounphrases()`: a one-step function to extract noun phrases without `spacy_parse()`

### When you finish

When `spacy_initialize()` is executed, a background process of spaCy is attached in python space. This can take up a significant size of memory especially when a larger language model is used (e.g. [en_core_web_lg](https://spacy.io/models/en#en_core_web_lg)). When you do not need the connection to spaCy any longer, you can remove the spaCy object by calling the `spacy_finalize()` function.

```{r, eval = FALSE}
spacy_finalize()
```
By calling `spacy_initialize()` again, you can reattach the backend spaCy.

## Using **spacyr** with other packages

### **quanteda**

Some of the token- and type-related standard methods from [**quanteda**](https://github.com/quanteda/quanteda) also work on the new tagged token objects:
```{r}
require(quanteda, warn.conflicts = FALSE, quietly = TRUE)
docnames(parsedtxt)
ndoc(parsedtxt)
ntoken(parsedtxt)
ntype(parsedtxt)
```

### Conformity to the *Text Interchange Format*

The [Text Interchange Format](https://github.com/ropensci/tif) is an emerging standard structure for text package objects in R, such as corpus and token objects.  `spacy_initialize()` can take a TIF corpus data.frame or character object as a valid input.  Moreover, the data.frames returned by `spacy_parse()` and `entity_consolidate()` conform to the TIF tokens standard for data.frame tokens objects.  This will make it easier to use with any text analysis package for R that works with TIF standard objects.
